
=== EXP10: DNN + reduced elementwise + nested CV (CV1) ===

Using device: cpu
Loaded df: (2449, 1658)
Interaction Type
antagonism    1227
synergy       1222
Name: count, dtype: int64

After filtering (binary): (2449, 1658)
Interaction Type
antagonism    1227
synergy       1222
Name: count, dtype: int64
Classes: ['antagonism', 'synergy']
Total samples: 2449
Num features: 1648

CV1 outer split (by Drug Pair)
Train: 1943 Test: 506

--- Nested CV: inner hyperparameter search ---

Config: {'hidden': (512, 256, 128), 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 0.0001}
  Fold 1...
    epoch 001 | train=0.5803 | val=1.0746
    epoch 002 | train=0.3501 | val=0.6421
    epoch 003 | train=0.2676 | val=0.6951
    epoch 004 | train=0.2190 | val=1.6180
    epoch 005 | train=0.2017 | val=1.0623
    epoch 006 | train=0.1967 | val=1.5300
    epoch 007 | train=0.1750 | val=0.4946
    epoch 008 | train=0.1766 | val=4.0422
    epoch 009 | train=0.1759 | val=0.8519
    epoch 010 | train=0.1532 | val=0.6296
    epoch 011 | train=0.1467 | val=3.2764
    epoch 012 | train=0.1489 | val=0.5448
    epoch 013 | train=0.1517 | val=1.5653
    epoch 014 | train=0.1361 | val=1.4620
    epoch 015 | train=0.1385 | val=0.6257
    epoch 016 | train=0.1320 | val=0.6330
    epoch 017 | train=0.1356 | val=0.9214
    early stopping
    Fold AUC=0.799, Acc=0.675
  Fold 2...
    epoch 001 | train=0.5889 | val=1.0066
    epoch 002 | train=0.3673 | val=0.7308
    epoch 003 | train=0.2939 | val=1.8708
    epoch 004 | train=0.2488 | val=1.5541
    epoch 005 | train=0.2502 | val=1.6972
    epoch 006 | train=0.2091 | val=1.4078
    epoch 007 | train=0.2096 | val=0.5399
    epoch 008 | train=0.1706 | val=0.6772
    epoch 009 | train=0.1683 | val=0.5643
    epoch 010 | train=0.1544 | val=1.0042
    epoch 011 | train=0.1556 | val=1.1247
    epoch 012 | train=0.1478 | val=1.2069
    epoch 013 | train=0.1551 | val=0.6891
    epoch 014 | train=0.1464 | val=0.5921
    epoch 015 | train=0.1603 | val=1.3943
    epoch 016 | train=0.1597 | val=0.5336
    epoch 017 | train=0.1501 | val=1.8704
    epoch 018 | train=0.1751 | val=1.6498
    epoch 019 | train=0.1552 | val=13.6649
    epoch 020 | train=0.1662 | val=1.8492
    epoch 021 | train=0.1546 | val=0.7163
    epoch 022 | train=0.1481 | val=0.6880
    epoch 023 | train=0.1386 | val=0.9289
    epoch 024 | train=0.1433 | val=0.6628
    epoch 025 | train=0.1455 | val=0.8823
    epoch 026 | train=0.1602 | val=1.4697
    early stopping
    Fold AUC=0.758, Acc=0.566
  Fold 3...
    epoch 001 | train=0.6058 | val=0.7940
    epoch 002 | train=0.3711 | val=0.6790
    epoch 003 | train=0.2861 | val=1.9099
    epoch 004 | train=0.2557 | val=0.6089
    epoch 005 | train=0.2273 | val=1.1919
    epoch 006 | train=0.2115 | val=0.4859
    epoch 007 | train=0.1985 | val=0.9121
    epoch 008 | train=0.2069 | val=1.3357
    epoch 009 | train=0.1888 | val=2.0276
    epoch 010 | train=0.1865 | val=0.6540
    epoch 011 | train=0.1711 | val=0.6037
    epoch 012 | train=0.1689 | val=0.4981
    epoch 013 | train=0.1734 | val=0.6138
    epoch 014 | train=0.1630 | val=1.9566
    epoch 015 | train=0.1620 | val=2.6244
    epoch 016 | train=0.1624 | val=1.0893
    early stopping
    Fold AUC=0.868, Acc=0.711
  --> Mean inner AUC=0.808, Acc=0.651

Config: {'hidden': (256, 128), 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 0.0003}
  Fold 1...
    epoch 001 | train=0.5783 | val=1.0274
    epoch 002 | train=0.3506 | val=0.6042
    epoch 003 | train=0.2619 | val=0.5200
    epoch 004 | train=0.2407 | val=0.5177
    epoch 005 | train=0.2139 | val=0.8757
    epoch 006 | train=0.1957 | val=0.5156
    epoch 007 | train=0.1865 | val=0.5471
    epoch 008 | train=0.1792 | val=2.6940
    epoch 009 | train=0.1716 | val=0.5108
    epoch 010 | train=0.1632 | val=0.5623
    epoch 011 | train=0.1712 | val=0.5733
    epoch 012 | train=0.1547 | val=0.7060
    epoch 013 | train=0.1618 | val=0.6202
    epoch 014 | train=0.1577 | val=1.2302
    epoch 015 | train=0.1482 | val=0.7776
    epoch 016 | train=0.1514 | val=1.2676
    epoch 017 | train=0.1437 | val=0.6287
    epoch 018 | train=0.1436 | val=0.6737
    epoch 019 | train=0.1408 | val=0.6777
    early stopping
    Fold AUC=0.836, Acc=0.747
  Fold 2...
    epoch 001 | train=0.5781 | val=0.9328
    epoch 002 | train=0.3660 | val=0.7031
    epoch 003 | train=0.2644 | val=0.6597
    epoch 004 | train=0.2429 | val=0.9093
    epoch 005 | train=0.2795 | val=0.6167
    epoch 006 | train=0.1996 | val=0.7267
    epoch 007 | train=0.1929 | val=0.5341
    epoch 008 | train=0.1846 | val=0.5199
    epoch 009 | train=0.1965 | val=2.4313
    epoch 010 | train=0.1784 | val=0.6535
    epoch 011 | train=0.1714 | val=4.1335
    epoch 012 | train=0.1869 | val=1.7504
    epoch 013 | train=0.1860 | val=0.5668
    epoch 014 | train=0.1609 | val=0.9382
    epoch 015 | train=0.1578 | val=0.5333
    epoch 016 | train=0.1525 | val=0.8127
    epoch 017 | train=0.1422 | val=0.7949
    epoch 018 | train=0.1389 | val=0.6620
    early stopping
    Fold AUC=0.844, Acc=0.773
  Fold 3...
    epoch 001 | train=0.5947 | val=0.8119
    epoch 002 | train=0.3628 | val=0.6149
    epoch 003 | train=0.2860 | val=0.7840
    epoch 004 | train=0.2684 | val=0.4940
    epoch 005 | train=0.2364 | val=0.5821
    epoch 006 | train=0.2270 | val=0.4765
    epoch 007 | train=0.2047 | val=0.8442
    epoch 008 | train=0.2209 | val=2.0954
    epoch 009 | train=0.2063 | val=1.4612
    epoch 010 | train=0.1946 | val=0.5462
    epoch 011 | train=0.1958 | val=0.5051
    epoch 012 | train=0.1903 | val=0.7305
    epoch 013 | train=0.1897 | val=0.4943
    epoch 014 | train=0.1771 | val=0.7006
    epoch 015 | train=0.1686 | val=1.4024
    epoch 016 | train=0.1665 | val=0.8954
    early stopping
    Fold AUC=0.855, Acc=0.638
  --> Mean inner AUC=0.845, Acc=0.719

Config: {'hidden': (512, 256), 'dropout': 0.4, 'lr': 0.0007, 'weight_decay': 0.0001}
  Fold 1...
    epoch 001 | train=0.5868 | val=0.8026
    epoch 002 | train=0.3354 | val=0.5811
    epoch 003 | train=0.2705 | val=0.7304
    epoch 004 | train=0.2549 | val=1.8369
    epoch 005 | train=0.2159 | val=0.5295
    epoch 006 | train=0.1951 | val=0.7933
    epoch 007 | train=0.2123 | val=0.5911
    epoch 008 | train=0.1733 | val=1.3232
    epoch 009 | train=0.1859 | val=0.7515
    epoch 010 | train=0.1800 | val=0.4930
    epoch 011 | train=0.1746 | val=0.6360
    epoch 012 | train=0.1630 | val=0.6270
    epoch 013 | train=0.1713 | val=0.5162
    epoch 014 | train=0.1629 | val=0.5886
    epoch 015 | train=0.1588 | val=0.5355
    epoch 016 | train=0.1579 | val=0.5624
    epoch 017 | train=0.1523 | val=0.6332
    epoch 018 | train=0.1588 | val=1.0547
    epoch 019 | train=0.1426 | val=0.6455
    epoch 020 | train=0.1521 | val=0.7724
    early stopping
    Fold AUC=0.819, Acc=0.697
  Fold 2...
    epoch 001 | train=0.5839 | val=1.0537
    epoch 002 | train=0.3573 | val=0.7597
    epoch 003 | train=0.2863 | val=0.7274
    epoch 004 | train=0.2268 | val=1.3373
    epoch 005 | train=0.2542 | val=0.4967
    epoch 006 | train=0.2163 | val=1.5048
    epoch 007 | train=0.2148 | val=0.7086
    epoch 008 | train=0.2242 | val=0.5085
    epoch 009 | train=0.2071 | val=0.7244
    epoch 010 | train=0.1966 | val=0.5732
    epoch 011 | train=0.2020 | val=0.5734
    epoch 012 | train=0.2401 | val=0.5289
    epoch 013 | train=0.1812 | val=0.5782
    epoch 014 | train=0.1984 | val=0.8252
    epoch 015 | train=0.1624 | val=0.5287
    early stopping
    Fold AUC=0.854, Acc=0.785
  Fold 3...
    epoch 001 | train=0.6077 | val=0.7996
    epoch 002 | train=0.3618 | val=0.6648
    epoch 003 | train=0.2977 | val=1.0940
    epoch 004 | train=0.2521 | val=0.5857
    epoch 005 | train=0.2568 | val=0.9610
    epoch 006 | train=0.2480 | val=0.5638
    epoch 007 | train=0.2243 | val=1.6679
    epoch 008 | train=0.2156 | val=0.5752
    epoch 009 | train=0.2130 | val=0.4770
    epoch 010 | train=0.2189 | val=0.5459
    epoch 011 | train=0.1908 | val=0.6264
    epoch 012 | train=0.1846 | val=0.4938
    epoch 013 | train=0.1842 | val=1.1681
    epoch 014 | train=0.1918 | val=0.7324
    epoch 015 | train=0.1852 | val=1.1901
    epoch 016 | train=0.1940 | val=0.6557
    epoch 017 | train=0.1863 | val=0.7828
    epoch 018 | train=0.1716 | val=0.5031
    epoch 019 | train=0.1753 | val=0.5809
    early stopping
    Fold AUC=0.862, Acc=0.756
  --> Mean inner AUC=0.845, Acc=0.746

Config: {'hidden': (256, 256, 128), 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 0.0001}
  Fold 1...
    epoch 001 | train=0.6156 | val=1.1246
    epoch 002 | train=0.3894 | val=0.7238
    epoch 003 | train=0.2840 | val=1.3176
    epoch 004 | train=0.2413 | val=0.5315
    epoch 005 | train=0.2248 | val=0.5082
    epoch 006 | train=0.1945 | val=1.0400
    epoch 007 | train=0.1952 | val=0.6043
    epoch 008 | train=0.1778 | val=0.5614
    epoch 009 | train=0.1734 | val=0.6293
    epoch 010 | train=0.1720 | val=0.5939
    epoch 011 | train=0.1531 | val=0.5258
    epoch 012 | train=0.1607 | val=0.9893
    epoch 013 | train=0.1469 | val=0.5157
    epoch 014 | train=0.1568 | val=0.5649
    epoch 015 | train=0.1463 | val=0.6041
    early stopping
    Fold AUC=0.786, Acc=0.719
  Fold 2...
    epoch 001 | train=0.6115 | val=1.1923
    epoch 002 | train=0.3837 | val=0.9890
    epoch 003 | train=0.2877 | val=0.5741
    epoch 004 | train=0.2644 | val=0.7006
    epoch 005 | train=0.2192 | val=0.6347
    epoch 006 | train=0.2079 | val=1.6814
    epoch 007 | train=0.2069 | val=0.5416
    epoch 008 | train=0.1888 | val=0.5938
    epoch 009 | train=0.1919 | val=0.5213
    epoch 010 | train=0.1716 | val=1.4335
    epoch 011 | train=0.1802 | val=0.8282
    epoch 012 | train=0.1720 | val=1.6522
    epoch 013 | train=0.1720 | val=0.8268
    epoch 014 | train=0.1650 | val=0.7104
    epoch 015 | train=0.1590 | val=0.5388
    epoch 016 | train=0.1482 | val=0.6458
    epoch 017 | train=0.2254 | val=0.5725
    epoch 018 | train=0.1689 | val=0.5793
    epoch 019 | train=0.2320 | val=1.9870
    early stopping
    Fold AUC=0.773, Acc=0.497
  Fold 3...
    epoch 001 | train=0.6291 | val=0.7996
    epoch 002 | train=0.3899 | val=1.1088
    epoch 003 | train=0.3153 | val=0.6531
    epoch 004 | train=0.2626 | val=0.9330
    epoch 005 | train=0.2391 | val=0.9155
    epoch 006 | train=0.2261 | val=0.5518
    epoch 007 | train=0.2081 | val=0.5469
    epoch 008 | train=0.2107 | val=0.7031
    epoch 009 | train=0.2062 | val=0.6283
    epoch 010 | train=0.1886 | val=0.5235
    epoch 011 | train=0.1811 | val=0.6237
    epoch 012 | train=0.1834 | val=0.5442
    epoch 013 | train=0.1676 | val=0.5920
    epoch 014 | train=0.1687 | val=0.5424
    epoch 015 | train=0.1726 | val=0.4887
    epoch 016 | train=0.1673 | val=0.6353
    epoch 017 | train=0.1613 | val=0.7396
    epoch 018 | train=0.1664 | val=0.5140
    epoch 019 | train=0.1596 | val=0.6426
    epoch 020 | train=0.1572 | val=0.5530
    epoch 021 | train=0.1555 | val=0.7147
    epoch 022 | train=0.1549 | val=0.5445
    epoch 023 | train=0.1574 | val=0.5275
    epoch 024 | train=0.1528 | val=0.8014
    epoch 025 | train=0.1500 | val=1.2077
    early stopping
    Fold AUC=0.839, Acc=0.649
  --> Mean inner AUC=0.799, Acc=0.622

Best config from nested CV:
Mean inner AUC: 0.845 Mean inner Acc: 0.719
Config: {'hidden': (256, 128), 'dropout': 0.3, 'lr': 0.001, 'weight_decay': 0.0003}

--- Final training on outer-train with best config ---

    epoch 001 | train=0.5920 | val=0.9813
    epoch 002 | train=0.3410 | val=0.8914
    epoch 003 | train=0.2639 | val=3.8870
    epoch 004 | train=0.2297 | val=0.8903
    epoch 005 | train=0.2348 | val=0.4588
    epoch 006 | train=0.2102 | val=3.0645
    epoch 007 | train=0.1903 | val=0.5592
    epoch 008 | train=0.1729 | val=0.5797
    epoch 009 | train=0.1720 | val=2.1715
    epoch 010 | train=0.1692 | val=0.6637
    epoch 011 | train=0.1670 | val=0.5245
    epoch 012 | train=0.1475 | val=0.7608
    epoch 013 | train=0.1579 | val=0.5806
    epoch 014 | train=0.1496 | val=0.6245
    epoch 015 | train=0.1405 | val=0.6615
    epoch 016 | train=0.1366 | val=0.6885
    epoch 017 | train=0.1441 | val=0.5385
    epoch 018 | train=0.1383 | val=0.5983
    epoch 019 | train=0.1475 | val=0.6411
    epoch 020 | train=0.1464 | val=0.8884
    early stopping

=== Evaluation on outer held-out test (pair-disjoint) ===

ROC AUC : 0.725
Acc     : 0.591
F1 (w)  : 0.536
F1 (mac): 0.529

Confusion matrix:
 [[ 58 185]
 [ 22 241]]

Classification report:
               precision    recall  f1-score   support

  antagonism       0.72      0.24      0.36       243
     synergy       0.57      0.92      0.70       263

    accuracy                           0.59       506
   macro avg       0.65      0.58      0.53       506
weighted avg       0.64      0.59      0.54       506


--- Metrics for Log ---
accuracy_test=0.5909
f1_macro_test=0.5293
f1_weighted_test=0.5361
roc_auc_test=0.7248
precision_antag=0.7250
recall_antag=0.2387
f1_antag=0.3591
precision_syn=0.5657
recall_syn=0.9163
f1_syn=0.6996

=== Overfitting check ===

Train AUC: 0.96 | Test AUC: 0.725
Train Acc: 0.835 | Test Acc: 0.591
Train F1w: 0.832 | Test F1w: 0.536
Train F1m: 0.832 | Test F1m: 0.529

=== EXP10 DONE ===

